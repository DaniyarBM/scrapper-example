{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "from lxml import html\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-161c818b9def>, line 82)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-161c818b9def>\"\u001b[1;36m, line \u001b[1;32m82\u001b[0m\n\u001b[1;33m    except:\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class Scrapper:\n",
    "    \n",
    "    def __init__(self, source, link_pattern, xpath, df, max_while = 4, max_links = 20):\n",
    "        self.source = source\n",
    "        self.link_pattern = link_pattern\n",
    "        self.xpath = xpath\n",
    "        self.df = df\n",
    "        self.max_while = max_while\n",
    "        self.max_links = max_links\n",
    "        self.link_storage = []\n",
    "        self.link_history = []\n",
    "        self.img = html.xpath\n",
    "    \n",
    "    def scrape(self, seed_link):\n",
    "        self.link_storage.append(seed_link)\n",
    "        print('Put link into seed link storage')\n",
    "        i = 0\n",
    "        while True:\n",
    "            links_list_length = len(self.link_storage)\n",
    "            print('Entered while cycle')\n",
    "            links = self.link_storage.copy()\n",
    "            for link in links:\n",
    "                print('Entered for cycle')\n",
    "                if link not in self.link_history:\n",
    "                    seed_page = self.load(link)\n",
    "                    print('Tried to load: ' + link)\n",
    "                    self.link_history.append(link)\n",
    "                    if seed_page:\n",
    "                        print('Loaded: ' + link)\n",
    "                        if self.is_needed(link):\n",
    "                            print('Link is needed: ' + link)\n",
    "                            self.extract_info(seed_page)\n",
    "                            print('Info extracted')\n",
    "                        self.extract_links(seed_page)\n",
    "                        print('Links extracted')\n",
    "                if self.df.shape[0] > self.max_links:\n",
    "                    break\n",
    "            if links_list_length == len(self.link_storage):\n",
    "                i += 1\n",
    "            if self.df.shape[0] > self.max_links or i > self.max_while:\n",
    "                break\n",
    "        #    self.scrape(self.link_storage)\n",
    "        return self.df\n",
    "        \n",
    "    \n",
    "    def extract_links(self, page):\n",
    "        pattern = self.source + '[a-z0-9\\-\\_\\.\\/]+'\n",
    "        links = re.findall(pattern, page.text, image.jpg,)\n",
    "        for link in links:\n",
    "            if not link in self.link_storage:\n",
    "                self.link_storage.append(link)\n",
    "    \n",
    "    def load(self, link):\n",
    "        response = requests.get(link)            \n",
    "        return response\n",
    "    \n",
    "    def extract_info(self, page):\n",
    "        tree = html.fromstring(page.content.decode('UTF-8'))\n",
    "        result = tree.xpath(self.xpath)\n",
    "        self.df = self.df.append(pd.DataFrame(result, columns=list(self.df.columns)), ignore_index=True)\n",
    "        return result\n",
    "    \n",
    "    def is_needed(self, link):\n",
    "        return bool(re.search(self.link_pattern, link))\n",
    "    \n",
    "    def get_images(url):\n",
    "        soup = make_soup(url)\n",
    "        images = [img for img in soup.findAll('img')]\n",
    "        print (str(len(images)) + \" images found.\")\n",
    "        print('Downloading images to current working directory.')\n",
    "        image_links = [each.get('src') for each in images]\n",
    "        for each in image_links:\n",
    "            try:\n",
    "                filename = each.strip().split('/')[-1].strip()\n",
    "                src = urljoin(url, each)\n",
    "                print('Getting: ' + filename)\n",
    "                response = requests.get(src, stream=True)\n",
    "                # delay to avoid corrupted previews\n",
    "                time.sleep(1)\n",
    "                with open(filename, 'wb') as out_file:\n",
    "                    shutil.copyfileobj(response.raw, out_file)\n",
    "                    except:\n",
    "                        print('  An error occured. Continuing.')\n",
    "                        print('Done.')\n",
    "                        if __name__ == '__main__':\n",
    "                            get_images('http://www.bbc.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'lxml.html' has no attribute 'xpath'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-1cfc7b038842>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#r = requests.get(url)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#html = lxml.html.fromstring(r.text)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhtml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"//*[@name='twitter:image:src']/@content\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'lxml.html' has no attribute 'xpath'"
     ]
    }
   ],
   "source": [
    "#url = 'http://prnt.sc/c0jkrl'\n",
    "\n",
    "#r = requests.get(url)\n",
    "#html = lxml.html.fromstring(r.text)\n",
    "img = html.xpath(\"//*[@name='twitter:image:src']/@content\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Scrapper' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-32687876ff32>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'title'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m scrapper_kloop = Scrapper('https://www.bbc.co.uk/news/world', \n\u001b[0m\u001b[0;32m      6\u001b[0m                           \u001b[1;34m'/news/world'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                           \u001b[1;34m'//h1/text()'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Scrapper' is not defined"
     ]
    }
   ],
   "source": [
    "#sys.setrecurtionLimit (100)\n",
    "\n",
    "df = pd.DataFrame(columns=['title'])\n",
    "\n",
    "scrapper_kloop = Scrapper('https://www.bbc.co.uk/news/world', \n",
    "                          '/news/world',\n",
    "                          '//h1/text()', \n",
    "                          '/image',\n",
    "                          df)\n",
    "\n",
    "#scrapper_kloop = Scrapper('https://kloop.kg',\n",
    "                          #'/blog/',\n",
    "                          #'//header/h1/text()', \n",
    "                         #df=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scrapper_kloop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-6f14fc6e0bfc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mscrapper_kloop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscrape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://www.bbc.com/news/world-middle-east-46088430 https://www.bbc.co.uk/news/world-middle-east-46088430'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'scrapper_kloop' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "scrapper_kloop.scrape('https://www.bbc.com/news/world-middle-east-46088430 https://www.bbc.co.uk/news/world-middle-east-46088430')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scrapper_kloop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-f30ba27380a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscrapper_kloop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://www.bbc.com/news/world-middle-east-46088430'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'scrapper_kloop' is not defined"
     ]
    }
   ],
   "source": [
    "page = scrapper_kloop.load('https://www.bbc.com/news/world-middle-east-46088430')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scrapper_kloop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-10154b6a07fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscrapper_kloop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'scrapper_kloop' is not defined"
     ]
    }
   ],
   "source": [
    "info = scrapper_kloop.extract_info(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scrapper_kloop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-51da92606a00>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscrapper_kloop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'scrapper_kloop' is not defined"
     ]
    }
   ],
   "source": [
    "scrapper_kloop.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scrapper_kloop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-000ee59f7254>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscrapper_kloop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'https://ichef.bbci.co.uk/news/1024/branded_news/0D80/production/_104165430_2594c607-ee56-4e59-bfc7-12965ed0c70c.jpg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'scrapper_kloop' is not defined"
     ]
    }
   ],
   "source": [
    "image = scrapper_kloop.load ('https://ichef.bbci.co.uk/news/1024/branded_news/0D80/production/_104165430_2594c607-ee56-4e59-bfc7-12965ed0c70c.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded_page = scrapper_kloop.load('https://kloop.kg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_l = scrapper_kloop.extract_links(scrapper_kloop.load(scrapper_kloop.source))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(list_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrapper_kloop.extract_info(loaded_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrapper_kloop.is_needed('https://kloop.kg/blog/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrapper_kloop.scrape(['https://kloop.kg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrapper_kloop.extract_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrapper_kloop.extract_links('''<head>\n",
    "    #<title>Новости Кыргызстана — Kloop.kg</title>\n",
    "    #<meta charset=\"UTF-8\" />\n",
    "    #<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    #<link rel=\"pingback\" href=\"https://kloop.kg/xmlrpc.php\" />\n",
    "    #\t\t\t\t<script>\n",
    "#\t\t\t\t\t\t\t\t</script>\n",
    "#\t\t\t<link rel=\"icon\" type=\"image/png\" href=\"https://kloop.kg/wp-content/uploads/2014/08/kloop_favicon.png\">\n",
    "#<meta property=\"og:url\" content=\"https://kloop.kg\" />\n",
    "#<meta property=\"og:type\" content=\"article\" />\n",
    "#<meta property=\"og:site_name\" content=\"KLOOP.KG - Новости Кыргызстана\" />\n",
    "#''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrapper_kloop.link_storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrapper_kloop.load('https://kloop.kg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrapper_kloop.scrape('https://kloop.kg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrapper_kloop.extract_info(page)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
